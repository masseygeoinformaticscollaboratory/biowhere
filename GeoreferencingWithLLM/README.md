# Fine-Tuning LLM Using QLoRA for for determining coordinates of New Zealand Biota from text

This directory contains a Jupyter notebook for fine-tuning the **Mistral-7B** model using **QLoRA**.

---

## ðŸ§® System Requirements

To fine-tune the Mistral-7B model efficiently, the following hardware and software specifications are recommended:

- **GPU:** NVIDIA A40-24Q (24 GiB VRAM or higher)
- **CPU:** 8 cores
- **RAM:** 64 GiB
- **Storage:** 200 GB HDD/SSD
- **CUDA Version:** 12.2

---

## ðŸ“¦ Dataset Requirements

You can use biological collection datasets from **GBIF** or other institutions, as long as each record includes the following columns:

- `locality`
- `decimalLatitude`
- `decimalLongitude`
- `countryCode`
- `stateProvince`

### ðŸ”€ Splitting the Dataset

To prepare your data for training, split it into **training**, **validation**, and **test** sets (e.g., 70:15:15 split) and save each split as a separate CSV file.

```python
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('your/file/path.csv')
train_df, temp_df = train_test_split(df, test_size=0.30, random_state=42)
val_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=42)

train_df.to_csv("train.csv", index=False)
val_df.to_csv("val.csv", index=False)
test_df.to_csv("test.csv", index=False)
```

---

## ðŸ”§ Fine-Tuning the Model

Follow these steps to fine-tune Mistral-7B:

1. Open [`fine_tune_mistral7B.ipynb`](fine_tune_mistral7B.ipynb).
2. Update the file paths to point to your train/validation/test datasets.
3. Run the notebook through **Step 5**. After completion, the fine-tuned model will be saved to the `./fine-tuned-model` directory.
4. Optionally, adjust hyperparameters to better suit your dataset.

> ðŸ’¡ You can also try different base models by changing the model reference in Step 2.

---

## ðŸ§ª Testing the Fine-Tuned Model

### 1. Load the Fine-Tuned Model

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

base_model_id = "mistralai/Mistral-7B-v0.1"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    quantization_config=bnb_config,
    device_map="cuda:0",
    trust_remote_code=True,
)

tokenizer = AutoTokenizer.from_pretrained(
    base_model_id,
    add_bos_token=True,
    trust_remote_code=True,
)

ft_model = PeftModel.from_pretrained(base_model, "./fine-tuned-model")
```

### 2. Run Inference

```python
from transformers import set_seed

ft_model.eval()

prompt = (
    "Task: Accurately georeference the location provided in the 'Locality Description' below, expressing the coordinates in decimal degrees."
    "\nContext: This 'Locality Description' refers to a location in Nelson, New Zealand."
    "\nLocality Description: Stockton Plateau 4.3 km south of Mt Frederick, stream near Deep Creek."
)

model_input = tokenizer(prompt, return_tensors="pt").to("cuda")

output = tokenizer.decode(
    ft_model.generate(
        **model_input,
        max_new_tokens=40,
        pad_token_id=tokenizer.eos_token_id
    )[0],
    skip_special_tokens=True
)

print(output)
```

The predicted latitude and longitude will be printed as output.

---
